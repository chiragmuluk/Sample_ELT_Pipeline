{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0a4f696-0b1c-4881-ad43-a282ab28250c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, lit,col,broadcast\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb6be862-ed91-4fb6-96f4-f0a1e2b84232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_last_seven_days_unprocessed_files(base_path,raw_table_name):\n",
    "    # Get today's date\n",
    "    today = datetime.today()\n",
    "\n",
    "    paths_to_check = []\n",
    "\n",
    "    # Generate folder paths for the last 7 days\n",
    "    for i in range(7):\n",
    "        date = today - timedelta(days=i)\n",
    "        folder_path = date.strftime(\"%Y/%m/%d\")\n",
    "        paths_to_check.append(f\"{base_path}/{folder_path}\")\n",
    "\n",
    "    existing_paths = []\n",
    "    # Check which paths exist in DBFS\n",
    "    for path in paths_to_check:\n",
    "        try:\n",
    "            if dbutils.fs.ls(path):\n",
    "                existing_paths.append(path)\n",
    "        except Exception as e:\n",
    "            # Skip paths that do not exist\n",
    "            print(f\"Path not found: {path}. Skipping...\")\n",
    "\n",
    "        # Read all files from existing paths as binary files\n",
    "        files_df = (spark.read\n",
    "            .format(\"binaryFile\")\n",
    "            .load(existing_paths))\n",
    "\n",
    "    # Filter for CSV files only\n",
    "    csv_files_df = files_df.filter(col(\"path\").endswith(\".csv\"))\n",
    "\n",
    "    # Get list of already processed file paths from the raw table\n",
    "    if spark.catalog.tableExists(raw_table_name):\n",
    "        processed_files_df = spark.table(raw_table_name).select(\"file_path\").distinct()\n",
    "    else:\n",
    "        processed_files_df = spark.createDataFrame([], \"file_path string\")\n",
    "\n",
    "    # Find new CSV files that have not been processed yet\n",
    "    new_files_df = csv_files_df.join(\n",
    "        broadcast(processed_files_df),\n",
    "        csv_files_df.path == processed_files_df.file_path,\n",
    "        \"left_anti\"\n",
    "    ).select(csv_files_df.path)\n",
    "\n",
    "    new_files_list = [row['path'] for row in new_files_df.collect()]\n",
    "    # Return DataFrame of new, unprocessed CSV file paths\n",
    "    return new_files_list"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "helpers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
