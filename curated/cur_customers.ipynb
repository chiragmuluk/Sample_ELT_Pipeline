{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffc981df-41ad-4295-a6dd-d69bcf1bd787",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import DeltaTable for performing merge operations on Delta tables\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Import necessary Spark SQL functions for data manipulation\n",
    "from pyspark.sql.functions import coalesce, current_timestamp, col, lit, max, sha2, concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ca9491-b8d3-43a1-ac44-ef66c091af40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utils/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffcf2af-f83d-4b5d-8faf-eb685be14526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleaned_customers_table=\"{}.{}\".format(enriched_uk_schema,cleaned_customers_table)\n",
    "dim_customers_table=\"{}.{}\".format(curated_uk_schema,dim_customers_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8036013-c436-48ea-9a20-e8ee6376bb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a default record with 'UNKNOWN' values for all fields\n",
    "default_record = {\n",
    "    \"customer_id\": \"UNKNOWN\",\n",
    "    \"first_name\": \"UNKNOWN\",\n",
    "    \"last_name\": \"UNKNOWN\",\n",
    "    \"email\": \"UNKNOWN\",\n",
    "    \"registration_date\": \"9999-12-01\",\n",
    "    \"city\": \"UNKNOWN\",\n",
    "    \"country\": \"UNKNOWN\",\n",
    "    \"load_date\": \"9999-12-01\",\n",
    "    \"source_system\": \"UNKNOWN\"\n",
    "}\n",
    "\n",
    "# Convert the default record to a Spark DataFrame\n",
    "default_df = spark.createDataFrame([default_record])\n",
    "\n",
    "# Add a customer_hash_key column using SHA-256 hash of 'UNKNOWN'\n",
    "default_df = default_df.withColumn(\n",
    "    \"customer_hash_key\",\n",
    "    sha2(concat_ws(\"^\", lit(\"UNKNOWN\")), 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0f4b9fa-8a79-416f-a82f-fde94c410e5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the maximum load_date from the target Delta table.\n",
    "# If no records exist, default to '1900-01-01'.\n",
    "max_load_date = (\n",
    "    spark.table(dim_customers_table)\n",
    "    .agg(\n",
    "        coalesce(\n",
    "            max(col(\"load_date\")),\n",
    "            lit(\"1900-01-01\")\n",
    "        ).alias(\"max_load_date\")\n",
    "    )\n",
    "    .collect()[0][\"max_load_date\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a43f92-63c1-46c7-aa34-7428879fedae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the source DataFrame from the enriched customers table\n",
    "source_df = (\n",
    "    spark.table(cleaned_customers_table)  # Read data from source table\n",
    "    .filter(col(\"is_current\") == True)  # Keep only current records\n",
    "    .withColumn(\"email\", coalesce(col(\"email\"), lit(\"UNKNOWN\")))  # Replace null emails with 'UNKNOWN'\n",
    "    .withColumn(\"load_date\", current_timestamp())  # Add current timestamp as load_date\n",
    "    .drop(\"start_date\", \"end_date\", \"is_current\",\"row_hash\")  # Remove unnecessary columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e8c44c-f97d-4cf1-ac51-4cc5fe116cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define DeltaTable for the target customers table\n",
    "target_table = DeltaTable.forName(spark,dim_customers_table)\n",
    "\n",
    "# Filter source_df to include only records with _processing_timestamp greater than max_load_date\n",
    "filtered_source_df = source_df.filter(col(\"_processing_timestamp\") > max_load_date)\n",
    "# Drop technical columns not needed in the target table\n",
    "filtered_source_df = filtered_source_df.drop(\"_processing_timestamp\", \"_source_file_path\")\n",
    "\n",
    "# Add the default record to ensure at least one row is present\n",
    "filtered_source_df = filtered_source_df.unionByName(default_df)\n",
    "\n",
    "# Perform a merge (upsert) operation from source to target Delta table\n",
    "target_table.alias(\"target\").merge(\n",
    "    filtered_source_df.alias(\"source\"),\n",
    "    \"target.customer_hash_key = source.customer_hash_key\"\n",
    ").whenMatchedUpdate(set={\n",
    "    # Update target fields when a match is found\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"first_name\": \"source.first_name\",\n",
    "    \"last_name\": \"source.last_name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"registration_date\": \"source.registration_date\",\n",
    "    \"city\": \"source.city\",\n",
    "    \"country\": \"source.country\",\n",
    "    \"load_date\": \"source.load_date\",\n",
    "    \"source_system\": \"source.source_system\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    # Insert new records when no match is found\n",
    "    \"customer_hash_key\": \"source.customer_hash_key\",\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"first_name\": \"source.first_name\",\n",
    "    \"last_name\": \"source.last_name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"registration_date\": \"source.registration_date\",\n",
    "    \"city\": \"source.city\",\n",
    "    \"country\": \"source.country\",\n",
    "    \"load_date\": \"source.load_date\",\n",
    "    \"source_system\": \"source.source_system\"\n",
    "}).execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8235478405599721,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "cur_customers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
