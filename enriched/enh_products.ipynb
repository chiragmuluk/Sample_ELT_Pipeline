{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cce43f9-1e3d-4b99-9600-b82c7101a55f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary PySpark SQL functions for DataFrame transformations and column operations\n",
    "from pyspark.sql.functions import col, when, lit, current_timestamp, trim, sha2, concat_ws, row_number\n",
    "\n",
    "# Import data types for casting columns\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Import window specification for deduplication logic\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Import DeltaTable for merge operations with Delta Lake\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47cfc164-085f-4535-841e-f4bc2336a64d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utils/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6dd3aa-8796-439f-a75b-1bd404bd6947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define table names for each layer in the pipeline\n",
    "bronze_table_name = \"{}.{}\".format(raw_uk_schema,raw_products_table)           # Raw product data (Bronze layer)\n",
    "silver_table_name = \"{}.{}\".format(enriched_uk_schema,cleaned_products_table)  # Cleaned and enriched product data (Silver layer)\n",
    "quarantine_table_name = \"{}.{}\".format(data_quality_uk_schema,data_quality_product_table)     # Invalid or quarantined product records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "310d67fd-f1b1-4e24-9c6f-0d7fc026c362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load today's product records from the Bronze table, filtering by creation date\n",
    "products_bronze_df = spark.table(bronze_table_name).filter(col(\"created_at\").cast(\"date\") == current_timestamp().cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa766f9-4e9a-42f8-9bef-7bf493605e0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a window specification to partition by product_id and order by created_at descending\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(col(\"created_at\").desc())\n",
    "\n",
    "# Add a row number to each record within the partition to identify the latest record\n",
    "deduped_df = products_bronze_df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the latest record for each product_id (row_num == 1)\n",
    "products_bronze_df = deduped_df.filter(\"row_num == 1\").drop(\"row_num\")\n",
    "\n",
    "# Drop unnecessary columns after deduplication\n",
    "products_bronze_df = products_bronze_df.drop(\"system_of_record\", \"created_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6fc167-31c8-4c46-904e-e01b2876b498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform deduped Bronze records for Silver layer processing\n",
    "products_silver_df = (\n",
    "    products_bronze_df\n",
    "    # Cast price to DoubleType for numeric operations\n",
    "    .withColumn(\"price\", col(\"price\").cast(DoubleType()))\n",
    "    # Cast stock_quantity to IntegerType for consistency\n",
    "    .withColumn(\"stock_quantity\", col(\"stock_quantity\").cast(IntegerType()))\n",
    "    # Add processing timestamp for audit and SCD2 logic\n",
    "    .withColumn(\"_processing_timestamp\", current_timestamp())\n",
    "    # Rename file_path to _source_file_path for lineage tracking\n",
    "    .withColumnRenamed(\"file_path\", \"_source_file_path\")\n",
    "    # Initialize error message column for validation results\n",
    "    .withColumn(\"_error_message\", lit(None).cast(\"string\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5080e927-83e6-4b95-88e3-77cc612dccf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean and standardize product_name and category fields\n",
    "products_silver_df = (\n",
    "    products_silver_df\n",
    "    # Set product_name to None if blank, otherwise trim whitespace\n",
    "    .withColumn(\n",
    "        \"product_name\",\n",
    "        when(\n",
    "            trim(col(\"product_name\")) == \"\",\n",
    "            lit(None)\n",
    "        ).otherwise(trim(col(\"product_name\")))\n",
    "    )\n",
    "    # Set category to None if null or blank, otherwise trim whitespace\n",
    "    .withColumn(\n",
    "        \"category\",\n",
    "        when(\n",
    "            col(\"category\").isNull() | (trim(col(\"category\")) == \"\"),\n",
    "            lit(None)\n",
    "        ).otherwise(trim(col(\"category\")))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522e1fb7-e5f2-4ec6-857c-2b728bf6131b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define validation rules as tuples for clarity\n",
    "# Each tuple contains a condition and the corresponding error message\n",
    "validation_rules = [\n",
    "    (col(\"price\").isNull() | (col(\"price\") <= 0), \"Price is missing or not a positive number.\"),\n",
    "    (col(\"product_id\").isNull(), \"Product ID is missing.\"),\n",
    "    (col(\"stock_quantity\").isNull(), \"Stock quantity is missing.\"),\n",
    "    (col(\"product_name\").isNull(), \"Product name is missing or blank.\")\n",
    "]\n",
    "\n",
    "# Build error message chain so only the first validation error is captured\n",
    "error_message_chain = lit(None)\n",
    "for condition, error in reversed(validation_rules):\n",
    "    error_message_chain = when(condition, error).otherwise(error_message_chain)\n",
    "\n",
    "# Add _error_message column to DataFrame based on validation rules\n",
    "products_silver_df = products_silver_df.withColumn(\n",
    "    \"_error_message\",\n",
    "    error_message_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b59721a-f0d4-46e6-86de-e47c0684aa3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Split records into valid and invalid sets based on validation results\n",
    "valid_records_df = products_silver_df.filter(col(\"_error_message\").isNull()).drop(\"_error_message\")  # Valid records: no error message\n",
    "invalid_records_df = products_silver_df.filter(col(\"_error_message\").isNotNull())  # Invalid records: error message present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f1c9dd-349d-4e6d-90b0-969acad319e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate a unique hash key for each product based on product_id\n",
    "valid_records_df = valid_records_df.withColumn(\"product_hash_key\", sha2(concat_ws(\n",
    "            \"^\",col(\"product_id\")\n",
    "            ),\n",
    "            256\n",
    "    )\n",
    ")\n",
    "\n",
    "# Generate a row-level hash to detect changes in price or stock_quantity\n",
    "valid_records_df = valid_records_df.withColumn(\"row_hash\",\n",
    "    sha2(\n",
    "        concat_ws(\n",
    "            \"^\",\n",
    "            col(\"price\"),\n",
    "            col(\"stock_quantity\")\n",
    "        ),\n",
    "        256\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589946f2-2e5c-48f2-a66c-41aebbb8334e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the Silver Delta table for SCD2 merge\n",
    "silver_delta_table = DeltaTable.forName(spark, silver_table_name.strip())\n",
    "\n",
    "# Define merge condition on product_hash_key\n",
    "merge_condition = \"target.product_hash_key = source.product_hash_key\"\n",
    "\n",
    "(\n",
    "    silver_delta_table.alias(\"target\")\n",
    "    # Merge valid records into Silver table using SCD2 logic\n",
    "    .merge(\n",
    "        valid_records_df.alias(\"source\"),\n",
    "        merge_condition\n",
    "    )\n",
    "    # When matched and row_hash differs, mark previous record as not current and set end_date\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.row_hash <> source.row_hash\",\n",
    "        set={\n",
    "            \"is_current\": \"false\",\n",
    "            \"end_date\": \"CAST(source._processing_timestamp AS DATE)\"\n",
    "        }\n",
    "    )\n",
    "    # When not matched, insert new record as current with start_date\n",
    "    .whenNotMatchedInsert(\n",
    "        values={\n",
    "            \"product_id\": \"source.product_id\",\n",
    "            \"product_name\": \"source.product_name\",\n",
    "            \"category\": \"source.category\",\n",
    "            \"price\": \"source.price\",\n",
    "            \"stock_quantity\": \"source.stock_quantity\",\n",
    "            \"product_hash_key\": \"source.product_hash_key\",\n",
    "            \"row_hash\": \"source.row_hash\",\n",
    "            \"source_system\": \"source.source_system\",\n",
    "            \"_source_file_path\": \"source._source_file_path\",\n",
    "            \"_processing_timestamp\":  \"CAST(source._processing_timestamp AS DATE)\",\n",
    "            \"is_current\": \"true\",\n",
    "            \"start_date\":  \"CAST(source._processing_timestamp AS DATE)\",\n",
    "            \"end_date\": \"null\"\n",
    "        }\n",
    "    )\n",
    "    .execute()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e5e0ad-e8ea-4308-a419-715f07b7241f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write invalid product records to the Quarantine Delta table for further review\n",
    "invalid_records_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(quarantine_table_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5130952999235155,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "enh_products",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
